{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package genesis to\n",
      "[nltk_data]     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\genesis.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('genesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Austen\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['companionableness',\n",
       " 'unceremoniousness',\n",
       " 'misunderstandings',\n",
       " 'misunderstandings',\n",
       " 'misunderstandings',\n",
       " 'misunderstandings',\n",
       " 'disinterestedness']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma = gutenberg.words('austen-emma.txt')\n",
    "#emma\n",
    "print(emma[4])\n",
    "longest_len = max([len(s) for s in emma])\n",
    "[s for s in emma if len(s) == longest_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('beat.n.01'), Synset('pulse.n.02'), Synset('rhythm.n.01'), Synset('beat.n.04'), Synset('beatnik.n.01'), Synset('beat.n.06'), Synset('meter.n.03'), Synset('beat.n.08'), Synset('beat.n.09'), Synset('beat.n.10'), Synset('beat.v.01'), Synset('beat.v.02'), Synset('beat.v.03'), Synset('beat.v.04'), Synset('beat.v.05'), Synset('drum.v.01'), Synset('beat.v.07'), Synset('beat.v.08'), Synset('beat.v.09'), Synset('beat.v.10'), Synset('beat.v.11'), Synset('beat.v.12'), Synset('beat.v.13'), Synset('tick.v.02'), Synset('beat.v.15'), Synset('beat.v.16'), Synset('pulsate.v.02'), Synset('beat.v.18'), Synset('beat.v.19'), Synset('beat.v.20'), Synset('outwit.v.01'), Synset('perplex.v.01'), Synset('exhaust.v.01'), Synset('all_in.s.01')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syns = wordnet.synsets('Beat')\n",
    "print(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('square.n.01'), Synset('square.n.02'), Synset('public_square.n.01'), Synset('square.n.04'), Synset('square.n.05'), Synset('square.n.06'), Synset('square.n.07'), Synset('square.n.08'), Synset('square.v.01'), Synset('square.v.02'), Synset('square.v.03'), Synset('square.v.04'), Synset('square.v.05'), Synset('square.v.06'), Synset('feather.v.03'), Synset('feather.v.04'), Synset('square.a.01'), Synset('straight.a.06'), Synset('hearty.s.02'), Synset('square.s.04'), Synset('square.s.05'), Synset('square.s.06'), Synset('squarely.r.02'), Synset('squarely.r.04'), Synset('squarely.r.03')]\n"
     ]
    }
   ],
   "source": [
    "syns = wordnet.synsets('square')\n",
    "print(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('squarely.r.02'), Synset('squarely.r.04'), Synset('squarely.r.03')]\n"
     ]
    }
   ],
   "source": [
    "syns = wordnet.synsets('square', pos=wordnet.ADV)\n",
    "print(syns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "president barack obama received the serve america act after congress' vote.\n",
      "he signed the bill last thursday. the president said it would greatly increase\n",
      "service opportunities for the american people.\n"
     ]
    }
   ],
   "source": [
    "text = '''President Barack Obama received the serve America act after congress' vote.\n",
    "He signed the bill last Thursday. The President said it would greatly increase\n",
    "service opportunities for the American People.'''\n",
    "lower_case_text = text.lower()\n",
    "print(lower_case_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['President', 'Barack', 'Obama', 'received', 'the', 'serve', 'America', 'act', 'after', 'congress', \"'\", 'vote', '.', 'He', 'signed', 'the', 'bill', 'last', 'Thursday', '.', 'The', 'President', 'said', 'it', 'would', 'greatly', 'increase', 'service', 'opportunities', 'for', 'the', 'American', 'People', '.']\n"
     ]
    }
   ],
   "source": [
    "wordtoken = []\n",
    "wordtoken = word_tokenize(text)\n",
    "print(wordtoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"President Barack Obama received the serve America act after congress' vote.\",\n",
       " 'He signed the bill last Thursday.',\n",
       " 'The President said it would greatly increase\\nservice opportunities for the American People.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = '''President Barack Obama received the serve America act after congress' vote.\n",
    "He signed the bill last Thursday. The President said it would greatly increase\n",
    "service opportunities for the American People.'''\n",
    "sentences = []\n",
    "sentences = sent_tokenize(context)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FinTechExplained', 'aims', 'to', 'explain', 'how', 'the', 'processing', 'works.', 'once', ',', 'we', 'have', 'gathered', 'the', 'text', ',', 'the', 'next', 'stage', 'is', 'about', 'cleaning', 'and', 'consolidating', 'the', 'text.', 'It', 'is', 'important', 'to', 'ensure', 'the', 'text', 'is', 'standardized', 'and', 'the', 'noise', 'is', 'remved', 'so', 'that', 'efficient', 'analysis', 'can', 'be', 'performed', 'on', 'the', 'text', 'to', 'derive', 'meaningful', 'insights']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "words = 'FinTechExplained aims to explain how the processing works. once, we have gathered the text, the next stage is about cleaning and consolidating the text. It is important to ensure the text is standardized and the noise is remved so that efficient analysis can be performed on the text to derive meaningful insights'\n",
    "print(tokenizer.tokenize(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PoS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading averaged_percetron_tagger: Package\n",
      "[nltk_data]     'averaged_percetron_tagger' not found in index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('President', 'NNP')]\n",
      "[('Barack', 'NN')]\n",
      "[('Obama', 'NN')]\n",
      "[('received', 'VBN')]\n",
      "[('the', 'DT')]\n",
      "[('serve', 'NN')]\n",
      "[('America', 'NNP')]\n",
      "[('act', 'NN')]\n",
      "[('after', 'IN')]\n",
      "[('congress', 'NN')]\n",
      "[(\"'\", \"''\")]\n",
      "[('vote', 'NN')]\n",
      "[('.', '.')]\n",
      "[('He', 'PRP')]\n",
      "[('signed', 'VBN')]\n",
      "[('the', 'DT')]\n",
      "[('bill', 'NN')]\n",
      "[('last', 'JJ')]\n",
      "[('Thursday', 'NNP')]\n",
      "[('.', '.')]\n",
      "[('The', 'DT')]\n",
      "[('President', 'NNP')]\n",
      "[('said', 'VBD')]\n",
      "[('it', 'PRP')]\n",
      "[('would', 'MD')]\n",
      "[('greatly', 'RB')]\n",
      "[('increase', 'NN')]\n",
      "[('service', 'NN')]\n",
      "[('opportunities', 'NNS')]\n",
      "[('for', 'IN')]\n",
      "[('the', 'DT')]\n",
      "[('American', 'JJ')]\n",
      "[('People', 'NNS')]\n",
      "[('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_percetron_tagger')\n",
    "for token in wordtoken:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\abhin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  President/NNP\n",
      "  (PERSON Barack/NNP Obama/NNP)\n",
      "  received/VBD\n",
      "  the/DT\n",
      "  serve/NN\n",
      "  (GPE America/NNP)\n",
      "  act/NN\n",
      "  after/IN\n",
      "  congress/NN\n",
      "  '/POS\n",
      "  vote/NN\n",
      "  ./.\n",
      "  He/PRP\n",
      "  signed/VBD\n",
      "  the/DT\n",
      "  bill/NN\n",
      "  last/JJ\n",
      "  Thursday/NNP\n",
      "  ./.\n",
      "  The/DT\n",
      "  President/NNP\n",
      "  said/VBD\n",
      "  it/PRP\n",
      "  would/MD\n",
      "  greatly/RB\n",
      "  increase/VB\n",
      "  service/NN\n",
      "  opportunities/NNS\n",
      "  for/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION American/JJ People/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "ne_tokens = word_tokenize(text)\n",
    "ne_tags = nltk.pos_tag(ne_tokens)\n",
    "NER = ne_chunk(ne_tags)\n",
    "print(NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization\n",
    "--> Stemming is the process of eliminating affixes from a word inorder to obtain a word stem.\n",
    "--> Lemmatization captures the canonical forms of the word's lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ship\n",
      "leav\n",
      "classif\n",
      "care\n",
      "thiev\n",
      "criteria\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "print(ps.stem(\"Shipping\"))\n",
    "print(ps.stem(\"leaves\"))\n",
    "print(ps.stem(\"classification\"))\n",
    "print(ps.stem(\"Caring\"))\n",
    "print(ps.stem(\"thieves\"))\n",
    "print(ps.stem(\"criteria\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ship\n",
      "leav\n",
      "class\n",
      "car\n",
      "thiev\n",
      "criter\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "ls = LancasterStemmer()\n",
    "\n",
    "print(ls.stem(\"Shipping\"))\n",
    "print(ls.stem(\"leaves\"))\n",
    "print(ls.stem(\"classification\"))\n",
    "print(ls.stem(\"Caring\"))\n",
    "print(ls.stem(\"thieves\"))\n",
    "print(ls.stem(\"criteria\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ship\n",
      "leav\n",
      "classif\n",
      "care\n",
      "thiev\n",
      "criteria\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "ss = SnowballStemmer('english')\n",
    "\n",
    "print(ss.stem(\"Shipping\"))\n",
    "print(ss.stem(\"leaves\"))\n",
    "print(ss.stem(\"classification\"))\n",
    "print(ss.stem(\"Caring\"))\n",
    "print(ss.stem(\"thieves\"))\n",
    "print(ss.stem(\"criteria\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thief\n",
      "leaf\n",
      "shipping\n",
      "criterion\n",
      "caring\n",
      "classification\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "print(wl.lemmatize('thieves'))\n",
    "print(wl.lemmatize('leaves'))\n",
    "print(wl.lemmatize('shipping'))\n",
    "print(wl.lemmatize('criteria'))\n",
    "print(wl.lemmatize('caring'))\n",
    "print(wl.lemmatize('classification'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'are', 'Reading', 'FinTexhExplained', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "punctuation = string.punctuation\n",
    "words = ['You', 'are', 'Reading', 'FinTexhExplained', '!', 'NLP', '.']\n",
    "clean_words = [w for w in words if w not in punctuation]\n",
    "print(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "['Lol', 'important', 'publications']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "text = 'Lol is an important publications'\n",
    "words = nltk.word_tokenize(text)\n",
    "stopwords = stopwords.words('english')\n",
    "print(stopwords)\n",
    "clean = [w for w in words if w not in stopwords]\n",
    "print(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', 'h', 'i', 's', ' ', 'i', 's', ' ', 'a', 'n', ' ', 'n', 'l', 'p', ' ', 'a', 'r', 't', 'i', 'c', 'l', 'e', ' ', 'o', 'f', ' ', 'l', 'o', 'l']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "SS = SnowballStemmer('english')\n",
    "\n",
    "text = 'This is an NLP Article of LOL'\n",
    "\n",
    "stem_words = [SS.stem(word) for word in text]\n",
    "print(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lol', 'is', 'an', 'important', 'publication']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "lemma_words = [wnl.lemmatize(word) for word in words]\n",
    "print(lemma_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N-Gram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('you', 'shall', 'know', 'a')\n",
      "('shall', 'know', 'a', 'word')\n",
      "('know', 'a', 'word', 'by')\n",
      "('a', 'word', 'by', 'the')\n",
      "('word', 'by', 'the', 'company')\n",
      "('by', 'the', 'company', 'it')\n",
      "('the', 'company', 'it', 'keeps')\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "n=4\n",
    "sentence = 'you shall know a word by the company it keeps'\n",
    "unigrams = ngrams(sentence.split(), n)\n",
    "for item in unigrams:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NLP',),\n",
       " ('NLP', 'is'),\n",
       " ('NLP', 'is', 'fun'),\n",
       " ('is',),\n",
       " ('is', 'fun'),\n",
       " ('fun',)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import everygrams\n",
    "message = 'NLP is fun'\n",
    "msg_split = message.split()\n",
    "\n",
    "list(everygrams(msg_split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating document similarity scores...\n",
      "\n",
      "Similarity Score [*]\n",
      " [[1.         0.46117093]]\n"
     ]
    }
   ],
   "source": [
    "#bring in standard stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "print(\"\\nCalculating document similarity scores...\")\n",
    "\n",
    "#open and read a bunch of files\n",
    "doc1 = 'Teaching machines to read and comprehend the relvant information from natural language documents is an elusive task.'\n",
    "doc2 = 'Teaching machines to read and comprehend is not an easy task'\n",
    "\n",
    "train_string = doc2\n",
    "train_set = [train_string, doc1]\n",
    "tfdif_vector = TfidfVectorizer(stop_words = stopwords)\n",
    "\n",
    "tfdif_matrix_train = tfdif_vector.fit_transform(train_set)\n",
    "print(\"\\nSimilarity Score [*]\\n\", cosine_similarity(tfdif_matrix_train[:1], tfdif_matrix_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, TreebankWordTokenizer, TweetTokenizer, MWETokenizer\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "from gensim.utils import tokenize\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# Sample text\n",
    "text = \"Tokenization is the process of breaking down text into smaller chunks such as words or sentences.\"\n",
    "\n",
    "# Word Tokenization\n",
    "print(\"Word Tokenization:\")\n",
    "print(word_tokenize(text))\n",
    "\n",
    "# Sentence Tokenization\n",
    "print(\"\\nSentence Tokenization:\")\n",
    "print(sent_tokenize(text))\n",
    "\n",
    "# Punctuation-based Tokenizer\n",
    "print(\"\\nPunctuation-based Tokenizer:\")\n",
    "print(nltk.tokenize.regexp_tokenize(text, pattern='\\w+|[^\\w\\s]'))\n",
    "\n",
    "# Treebank Word tokenizer\n",
    "print(\"\\nTreebank Word tokenizer:\")\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize(text))\n",
    "\n",
    "# Tweet Tokenizer\n",
    "print(\"\\nTweet Tokenizer:\")\n",
    "tknzr = TweetTokenizer()\n",
    "print(tknzr.tokenize(text))\n",
    "\n",
    "# Multi-Word Expression Tokenizer\n",
    "print(\"\\nMulti-Word Expression Tokenizer:\")\n",
    "mwe_tokenizer = MWETokenizer([('process', 'of')])\n",
    "print(mwe_tokenizer.tokenize(text.split()))\n",
    "\n",
    "# TextBlob Word Tokenize\n",
    "print(\"\\nTextBlob Word Tokenize:\")\n",
    "blob = TextBlob(text)\n",
    "print(blob.words)\n",
    "\n",
    "# spaCy Tokenizer\n",
    "print(\"\\nspaCy Tokenizer:\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "print([token.text for token in doc])\n",
    "\n",
    "# Gensim word tokenizer\n",
    "print(\"\\nGensim word tokenizer:\")\n",
    "print(list(tokenize(text, lowercase=True)))\n",
    "\n",
    "# Tokenization with Keras\n",
    "print(\"\\nTokenization with Keras:\")\n",
    "print(text_to_word_sequence(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog. However, the lazy dog doesn't seem to care. \n",
    "The fox and the dog are good friends. They often play together in the forest. \n",
    "But one day, the fox went missing. It was a sad day for the dog.\n",
    "\"\"\"\n",
    "\n",
    "# a. Count the number of unique tokens in the text.\n",
    "tokens = word_tokenize(text)\n",
    "unique_tokens = set(tokens)\n",
    "print(\"Number of unique tokens:\", len(unique_tokens))\n",
    "\n",
    "# b\n",
    "punctuation_count = sum([1 for char in text if char in string.punctuation])\n",
    "text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "#c\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(text)\n",
    "stopword_distribution = nltk.FreqDist(word for word in word_tokens if word in stop_words)\n",
    "stopword_distribution.plot()\n",
    "\n",
    "# d\n",
    "filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "filtered_text = ' '.join(filtered_text)\n",
    "\n",
    "# e\n",
    "pos_tags = pos_tag(word_tokens)\n",
    "pos_distribution = nltk.FreqDist(tag for (word, tag) in pos_tags)\n",
    "pos_distribution.plot()\n",
    "\n",
    "# f\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "unique_lemmas = set(lemmas)\n",
    "print(\"Number of unique lemmas:\", len(unique_lemmas))\n",
    "\n",
    "# g\n",
    "fdist = FreqDist(filtered_text.split())\n",
    "fdist.plot(10)\n",
    "\n",
    "# h\n",
    "bigrams = nltk.ngrams(filtered_text.split(), 2)\n",
    "trigrams = nltk.ngrams(filtered_text.split(), 3)\n",
    "quadgrams = nltk.ngrams(filtered_text.split(), 4)\n",
    "print(\"Number of unique bigrams:\", len(set(bigrams)))\n",
    "print(\"Number of unique trigrams:\", len(set(trigrams)))\n",
    "print(\"Number of unique quadgrams:\", len(set(quadgrams)))\n",
    "\n",
    "# i\n",
    "dates = re.findall(r'\\b\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\\b', text)\n",
    "formatted_dates = [datetime.strptime(date, '%d-%m-%Y').strftime('%d-%m-%Y') for date in dates]\n",
    "print(\"Formatted Dates:\", formatted_dates)\n",
    "\n",
    "# j\n",
    "years = [int(re.findall(r'\\b\\d{4}\\b', date)[0]) for date in dates]\n",
    "year_distribution = nltk.FreqDist(years)\n",
    "year_distribution.plot()\n",
    "\n",
    "# k\n",
    "phone_numbers = re.findall(r'\\b(?:\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})\\b', text)\n",
    "print(\"Phone Numbers:\", phone_numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.771651025060578\n",
      "Levenshtein Distance: 10\n",
      "Jaccard Index: 0.6363636363636364\n",
      "Euclidean Distance: 0.6757943103332876\n",
      "Hamming Distance: 0.023255813953488413\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import Levenshtein\n",
    "import textdistance\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "# Sample documents\n",
    "doc1 = \"The quick brown fox jumps over the lazy dog\"\n",
    "doc2 = \"A quick brown dog jumps on the lazy fox\"\n",
    "\n",
    "# Tokenize the documents\n",
    "tokens_doc1 = nltk.word_tokenize(doc1.lower())\n",
    "tokens_doc2 = nltk.word_tokenize(doc2.lower())\n",
    "\n",
    "# Cosine Similarity\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([doc1, doc2])\n",
    "cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)\n",
    "print(\"Cosine Similarity:\", cosine_sim[0][1])\n",
    "\n",
    "# Levenshtein Distance\n",
    "lev_distance = Levenshtein.distance(doc1, doc2)\n",
    "print(\"Levenshtein Distance:\", lev_distance)\n",
    "\n",
    "# Jaccard Index\n",
    "jaccard_index = textdistance.jaccard.normalized_similarity(tokens_doc1, tokens_doc2)\n",
    "print(\"Jaccard Index:\", jaccard_index)\n",
    "\n",
    "# Euclidean Distance\n",
    "# Convert documents to vectors\n",
    "vector1 = tfidf_vectorizer.transform([doc1]).toarray()\n",
    "vector2 = tfidf_vectorizer.transform([doc2]).toarray()\n",
    "euclidean_distance = np.linalg.norm(vector1 - vector2)\n",
    "print(\"Euclidean Distance:\", euclidean_distance)\n",
    "\n",
    "# Hamming Distance\n",
    "hamming_distance = textdistance.hamming.normalized_similarity(doc1, doc2)\n",
    "print(\"Hamming Distance:\", hamming_distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.9116846116771038\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Sample documents\n",
    "doc1 = \"The quick brown fox jumps over the lazy dog\"\n",
    "doc2 = \"A quick brown dog jumps on the lazy fox\"\n",
    "\n",
    "# Tokenize the documents\n",
    "tokens_doc1 = [doc1]\n",
    "tokens_doc2 = [doc2]\n",
    "\n",
    "# Create the vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "vector1 = vectorizer.fit_transform(tokens_doc1)\n",
    "vector2 = vectorizer.transform(tokens_doc2)\n",
    "\n",
    "# Check similarity\n",
    "similarity = cosine_similarity(vector1, vector2)\n",
    "print(\"Cosine Similarity:\", similarity[0][0])  # Accessing the single similarity score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 1.0000000000000002\n",
      "Levenshtein Distance: 4\n",
      "Jaccard Index: 1.0\n",
      "Euclidean Distance: 0.0\n",
      "Hamming Distance: 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import Levenshtein\n",
    "import textdistance\n",
    "import numpy as np\n",
    "\n",
    "# Sample documents\n",
    "doc1 = \"The quick brown fox jumps over the lazy dog\"\n",
    "doc2 = \"A quick brown dog jumps on the lazy fox\"\n",
    "\n",
    "# Tokenize the documents\n",
    "tokens_doc1 = nltk.word_tokenize(doc1.lower())\n",
    "tokens_doc2 = nltk.word_tokenize(doc2.lower())\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens_doc1 = [token for token in tokens_doc1 if token not in stop_words]\n",
    "tokens_doc2 = [token for token in tokens_doc2 if token not in stop_words]\n",
    "\n",
    "# Vectorize the tokens\n",
    "vectorizer = TfidfVectorizer()\n",
    "vector1 = vectorizer.fit_transform([' '.join(tokens_doc1)])\n",
    "vector2 = vectorizer.transform([' '.join(tokens_doc2)])\n",
    "\n",
    "# Cosine Similarity\n",
    "cosine_sim = cosine_similarity(vector1, vector2)[0][0]\n",
    "print(\"Cosine Similarity:\", cosine_sim)\n",
    "\n",
    "# Levenshtein Distance\n",
    "lev_distance = Levenshtein.distance(' '.join(tokens_doc1), ' '.join(tokens_doc2))\n",
    "print(\"Levenshtein Distance:\", lev_distance)\n",
    "\n",
    "# Jaccard Index\n",
    "jaccard_index = textdistance.jaccard.normalized_similarity(tokens_doc1, tokens_doc2)\n",
    "print(\"Jaccard Index:\", jaccard_index)\n",
    "\n",
    "# Euclidean Distance\n",
    "euclidean_distance = np.linalg.norm(vector1.toarray() - vector2.toarray())\n",
    "print(\"Euclidean Distance:\", euclidean_distance)\n",
    "\n",
    "# Hamming Distance\n",
    "hamming_distance = textdistance.hamming.normalized_similarity(' '.join(tokens_doc1), ' '.join(tokens_doc2))\n",
    "print(\"Hamming Distance:\", hamming_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Ensure NLTK and WordNet are downloaded\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if not synsets:\n",
    "        return None\n",
    "    synonyms = []\n",
    "    for synset in synsets:\n",
    "        synonyms.extend(synset.lemmas())\n",
    "    return set(synonyms)\n",
    "\n",
    "def get_definitions(word):\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if not synsets:\n",
    "        return None\n",
    "    return [syn.definition() for syn in synsets]\n",
    "\n",
    "def get_hypernyms(word):\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if not synsets:\n",
    "        return None\n",
    "    hypernyms = []\n",
    "    for synset in synsets:\n",
    "        for hypernym in synset.hypernyms():\n",
    "            hypernyms.extend(hypernym.lemma_names())\n",
    "    return set(hypernyms)\n",
    "\n",
    "def get_hyponyms(word):\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if not synsets:\n",
    "        return None\n",
    "    hyponyms = []\n",
    "    for synset in synsets:\n",
    "        for hyponym in synset.hyponyms():\n",
    "            hyponyms.extend(hyponym.lemma_names())\n",
    "    return set(hyponyms)\n",
    "\n",
    "def get_similarity(word1, word2):\n",
    "    synsets1 = wordnet.synsets(word1)\n",
    "    synsets2 = wordnet.synsets(word2)\n",
    "    if not synsets1 or not synsets2:\n",
    "        return None\n",
    "    max_similarity = 0\n",
    "    for synset1 in synsets1:\n",
    "        for synset2 in synsets2:\n",
    "            similarity = synset1.wup_similarity(synset2)\n",
    "            if similarity is not None and similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "    return max_similarity\n",
    "\n",
    "def display_menu():\n",
    "    print(\"Menu:\")\n",
    "    print(\"1. Find the 3rd meaning of the word\")\n",
    "    print(\"2. Extract nouns of the word\")\n",
    "    print(\"3. Extract verbs of the word\")\n",
    "    print(\"4. Extract adjectives of the word\")\n",
    "    print(\"5. Extract adverbs of the word\")\n",
    "    print(\"6. Extract the definition of the word\")\n",
    "    print(\"7. Find the hypernyms of the word\")\n",
    "    print(\"8. Find the hyponyms of the word\")\n",
    "    print(\"9. Find the similarity between any two hyponyms of the word\")\n",
    "    print(\"0. Exit\")\n",
    "\n",
    "def main():\n",
    "    word = input(\"Enter a word: \")\n",
    "    choice = None\n",
    "\n",
    "    while choice != '0':\n",
    "        display_menu()\n",
    "        choice = input(\"Enter your choice: \")\n",
    "\n",
    "        if choice == '1':\n",
    "            synonyms = get_synonyms(word)\n",
    "            if synonyms:\n",
    "                third_meaning = list(synonyms)[2]\n",
    "                print(\"Third meaning of '{}' is: {}\".format(word, third_meaning.name()))\n",
    "            else:\n",
    "                print(\"No synonyms found for '{}'.\".format(word))\n",
    "\n",
    "        elif choice == '2':\n",
    "            nouns = [lemma.name() for lemma in wordnet.synsets(word)[0].lemmas() if lemma.synset().pos() == 'n']\n",
    "            print(\"Nouns of '{}' are: {}\".format(word, nouns))\n",
    "\n",
    "        elif choice == '3':\n",
    "            verbs = [lemma.name() for lemma in wordnet.synsets(word)[0].lemmas() if lemma.synset().pos() == 'v']\n",
    "            print(\"Verbs of '{}' are: {}\".format(word, verbs))\n",
    "\n",
    "        elif choice == '4':\n",
    "            adjectives = [lemma.name() for lemma in wordnet.synsets(word)[0].lemmas() if lemma.synset().pos() == 'a']\n",
    "            print(\"Adjectives of '{}' are: {}\".format(word, adjectives))\n",
    "\n",
    "        elif choice == '5':\n",
    "            adverbs = [lemma.name() for lemma in wordnet.synsets(word)[0].lemmas() if lemma.synset().pos() == 'r']\n",
    "            print(\"Adverbs of '{}' are: {}\".format(word, adverbs))\n",
    "\n",
    "        elif choice == '6':\n",
    "            definitions = get_definitions(word)\n",
    "            if definitions:\n",
    "                print(\"Definition of '{}' is: {}\".format(word, definitions[0]))\n",
    "            else:\n",
    "                print(\"No definitions found for '{}'.\".format(word))\n",
    "\n",
    "        elif choice == '7':\n",
    "            hypernyms = get_hypernyms(word)\n",
    "            if hypernyms:\n",
    "                print(\"Hypernyms of '{}' are: {}\".format(word, hypernyms))\n",
    "            else:\n",
    "                print(\"No hypernyms found for '{}'.\".format(word))\n",
    "\n",
    "        elif choice == '8':\n",
    "            hyponyms = get_hyponyms(word)\n",
    "            if hyponyms:\n",
    "                print(\"Hyponyms of '{}' are: {}\".format(word, hyponyms))\n",
    "            else:\n",
    "                print(\"No hyponyms found for '{}'.\".format(word))\n",
    "\n",
    "        elif choice == '9':\n",
    "            hyponyms = get_hyponyms(word)\n",
    "            if hyponyms:\n",
    "                hyponyms_list = list(hyponyms)\n",
    "                similarity = get_similarity(hyponyms_list[0], hyponyms_list[1])\n",
    "                if similarity:\n",
    "                    print(\"Similarity between '{}' and '{}' is: {}\".format(hyponyms_list[0], hyponyms_list[1], similarity))\n",
    "                else:\n",
    "                    print(\"No hyponyms found for similarity comparison.\")\n",
    "            else:\n",
    "                print(\"No hyponyms found for similarity comparison.\")\n",
    "\n",
    "        elif choice == '0':\n",
    "            print(\"Exiting...\")\n",
    "\n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter a number from the menu.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
